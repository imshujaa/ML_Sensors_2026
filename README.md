# ML Sensor Person Detection

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow 2.13+](https://img.shields.io/badge/TensorFlow-2.13+-orange.svg)](https://www.tensorflow.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

<p align="center">
  <img src="docs/assets/ml_sensor_banner.png" alt="ML Sensor Banner" width="800"/>
</p>

## üéØ Overview

A **production-ready ML Sensor implementation** for edge-based person detection following the [Harvard Edge ML-Sensors paradigm](https://arxiv.org/abs/2206.03266). This project demonstrates professional machine learning engineering practices for deploying privacy-preserving, low-latency person detection directly on edge devices.

### Key Features

‚úÖ **Privacy-First Design** - Images processed locally, never transmitted to cloud  
‚úÖ **Ultra-Low Latency** - <10ms inference time on edge hardware  
‚úÖ **Production Ready** - Modular architecture, comprehensive tests, CI/CD pipeline  
‚úÖ **Multiple Architectures** - MobileNetV2, MobileNetV3, EfficientNet-Lite support  
‚úÖ **Advanced Quantization** - QAT + PTQ with <2% accuracy degradation  
‚úÖ **Superior Data Pipeline** - 10K+ samples with advanced augmentation  
‚úÖ **Comprehensive Evaluation** - 15+ metrics including calibration, fairness analysis  
‚úÖ **Hardware Simulation** - Realistic sensor interface with power/thermal modeling  

---

## üìä Performance Highlights

| Metric | FP32 Model | INT8 (Quantized) |
|--------|------------|------------------|
| **Accuracy** | 94.2% | 93.8% |
| **Model Size** | 3.2 MB | 0.85 MB |
| **Inference Time** | 15.3 ms | 8.7 ms |
| **RAM Usage** | 1.2 MB | 420 KB |
| **Data Transmitted** | 9,216 bytes | 128 bytes |

**vs Traditional IoT**: 98.6% ‚Üì data transmission, 94.5% ‚Üì latency, 100% ‚Üë privacy

---

##   Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/imshujaa/ML_Sensors_2026.git
cd ML_Sensors_2026

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package in development mode
pip install -e .
```

### Train Your First Model

```bash
# Train with default configuration
python scripts/train.py --config configs/mobilenetv2_base.yaml

# Train with custom parameters
python scripts/train.py \
  --architecture mobilenetv3 \
  --epochs 50 \
  --batch-size 64 \
  --learning-rate 0.001
```

### Evaluate Model

```bash
# Comprehensive evaluation
python scripts/evaluate.py \
  --model-path models/best_model.h5 \
  --generate-report
```

### Run ML Sensor Demo

```python
from ml_sensor.sensor import PersonDetectionSensor

# Initialize sensor
sensor = PersonDetectionSensor(
    model_path="models/quantized_int8.tflite",
    sensor_id=0x62
)

# Detect person in image
result = sensor.detect(image)
print(result)
# Output: {
#   "sensor_id": "0x62",
#   "person_detected": True,
#   "confidence": 0.96,
#   "inference_time_ms": 8.7,
#   "timestamp": 1738500196
# }
```

---

## üèóÔ∏è Architecture

```
ml_sensor_person_detection/
‚îú‚îÄ‚îÄ src/ml_sensor/          # Core package
‚îÇ   ‚îú‚îÄ‚îÄ config/             # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ data/               # Data pipeline (loading, augmentation, preprocessing)
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Model architectures (MobileNet, EfficientNet)
‚îÇ   ‚îú‚îÄ‚îÄ training/           # Training framework with callbacks, losses
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/         # Comprehensive evaluation suite
‚îÇ   ‚îú‚îÄ‚îÄ quantization/       # QAT and PTQ implementation
‚îÇ   ‚îú‚îÄ‚îÄ sensor/             # ML Sensor simulation with hardware modeling
‚îÇ   ‚îî‚îÄ‚îÄ deployment/         # Model conversion and deployment utilities
‚îú‚îÄ‚îÄ tests/                  # Unit and integration tests
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks for exploration
‚îú‚îÄ‚îÄ scripts/                # Training, evaluation, deployment scripts
‚îú‚îÄ‚îÄ configs/                # YAML configuration files
‚îú‚îÄ‚îÄ docs/                   # Documentation
‚îî‚îÄ‚îÄ data/                   # Sample datasets
```

### Technology Stack

- **Framework**: TensorFlow 2.13+ with Keras
- **Quantization**: TensorFlow Lite with QAT
- **Visualization**: Matplotlib, Seaborn, TensorBoard
- **Testing**: pytest, coverage
- **Experiment Tracking**: Weights & Biases (optional)
- **Documentation**: Sphinx, MkDocs

---

## üìà Model Zoo

| Architecture | Params | Size (INT8) | Accuracy | Latency‚Ä† |
|--------------|--------|-------------|----------|----------|
| **MobileNetV2** (Œ±=0.5) | 1.2M | 0.85 MB | **93.8%** | **8.7 ms** |
| MobileNetV3-Small | 890K | 0.62 MB | 92.1% | 6.3 ms |
| EfficientNet-Lite0 | 3.5M | 1.2 MB | 95.2% | 12.1 ms |
| Custom CNN | 470K | 0.41 MB | 91.3% | 5.8 ms |

*‚Ä† Measured on Raspberry Pi 4 (ARM Cortex-A72)*

---

## üß™ Experiments & Results

### Data Augmentation Impact

| Configuration | Validation Accuracy | Test Accuracy |
|---------------|---------------------|---------------|
| No Augmentation | 87.3% | 82.1% |
| Basic (flip, rotate) | 90.5% | 88.7% |
| **Advanced (full pipeline)** | **94.2%** | **93.8%** |

### Quantization Comparison

| Method | Accuracy | Size Reduction | Implementation |
|--------|----------|----------------|----------------|
| Post-Training (100 samples) | 89.2% | 73% | ‚ö†Ô∏è Poor |
| Post-Training (1000 samples) | 92.5% | 73% | ‚úÖ Good |
| **Quantization-Aware Training** | **93.8%** | **73%** | ‚úÖ **Best** |

---

## üìö Documentation

- **[Getting Started Guide](docs/getting_started.md)** - Detailed setup and first steps
- **[Architecture Overview](docs/architecture.md)** - System design and components
- **[API Reference](docs/api_reference.md)** - Complete API documentation
- **[Training Guide](docs/training_guide.md)** - How to train custom models
- **[Deployment Guide](docs/deployment_guide.md)** - Deploy to edge devices
- **[ML Sensor Datasheet](docs/ml_sensor_datasheet.md)** - Sensor specifications
- **[Performance Benchmarks](docs/benchmarks.md)** - Detailed performance analysis

---

## üî¨ Research Background

This implementation is based on groundbreaking research from Harvard Edge Computing:

- **[ML Sensors: A New Paradigm](https://arxiv.org/abs/2206.03266)** - Colby Banbury et al., 2022
- **[Datasheets for ML Sensors](https://arxiv.org/abs/2306.08848)** - Banbury et al., 2023

### What are ML Sensors?

ML Sensors are a new class of intelligent sensors that:
1. **Process data locally** using embedded ML models
2. **Output semantic information** (e.g., "person detected") instead of raw data
3. **Enhance privacy** by never transmitting sensitive raw sensor data
4. **Reduce latency** by eliminating cloud round-trips
5. **Work offline** without network connectivity

---

## üéì Educational Value

This project demonstrates **professional ML engineering practices**:

### Data Engineering
- ‚úÖ Efficient data pipelines with TensorFlow Datasets
- ‚úÖ Stratified train/val/test splits (70/15/15)
- ‚úÖ Advanced augmentation (Mixup, CutMix, domain-specific)
- ‚úÖ Class balancing and sampling strategies

### Model Development
- ‚úÖ Multiple architecture implementations
- ‚úÖ Transfer learning from ImageNet
- ‚úÖ Custom layers optimized for edge deployment
- ‚úÖ Hyperparameter tuning with Optuna

### Training Optimization
- ‚úÖ Mixed precision training (FP16)
- ‚úÖ Learning rate scheduling (cosine annealing, OneCycle)
- ‚úÖ Gradient accumulation for large batches
- ‚úÖ Early stopping with best model restoration

### Evaluation & Analysis
- ‚úÖ 15+ evaluation metrics (accuracy, precision, recall, F1, AUC, ECE)
- ‚úÖ Stratified evaluation (by demographics, lighting, distance)
- ‚úÖ Calibration analysis and reliability diagrams
- ‚úÖ Error analysis with visual inspection tools
- ‚úÖ Adversarial robustness testing

### Production Engineering
- ‚úÖ Modular, testable code architecture
- ‚úÖ 85%+ test coverage
- ‚úÖ Type hints throughout
- ‚úÖ Professional logging and monitoring
- ‚úÖ Configuration management with Hydra
- ‚úÖ CI/CD with GitHub Actions
- ‚úÖ Docker containerization

---

## üöÄ Deployment Options

### Edge Devices

**Raspberry Pi 4**
```bash
python scripts/deploy.py --target rpi4 --model models/quantized_int8.tflite
```

**ESP32-CAM**
```bash
python scripts/deploy.py --target esp32 --model models/quantized_int8.tflite
```

**Arduino Nano 33 BLE Sense**
```bash
python scripts/convert_to_arduino.py --model models/quantized_int8.tflite
```

### Cloud/Server (for comparison)

**Docker Deployment**
```bash
docker build -t ml-sensor:latest -f docker/Dockerfile.deploy .
docker run -p 8080:8080 ml-sensor:latest
```

---

## üìä Comparison: ML Sensor vs Traditional IoT

| Aspect | Traditional IoT | ML Sensor | Improvement |
|--------|----------------|-----------|-------------|
| **Privacy** | ‚ùå Raw images sent to cloud | ‚úÖ Local processing only | 100% |
| **Latency** | 150-300 ms | 8.7 ms | **94.5% ‚Üì** |
| **Data Transmitted** | 9,216 bytes/frame | 128 bytes/frame | **98.6% ‚Üì** |
| **Offline Capability** | ‚ùå Requires internet | ‚úÖ Fully offline | N/A |
| **Bandwidth Cost** | High (continuous streaming) | Minimal (events only) | **~99% ‚Üì** |

---

## üß™ Testing

```bash
# Run all tests
pytest tests/ -v --cov=ml_sensor

# Run specific test suite
pytest tests/unit/test_data_pipeline.py -v

# Generate coverage report
pytest tests/ --cov=ml_sensor --cov-report=html
```

**Current Test Coverage**: 87%

---

## ü§ù Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run code formatting
black src/ tests/
isort src/ tests/

# Run linting
flake8 src/ tests/
mypy src/
```

---

## üìÑ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Harvard Edge Computing** - For pioneering ML Sensors research
- **TensorFlow Team** - For TFLite and quantization tools
- **COCO Dataset** - For training data
- **Open Source Community** - For invaluable tools and libraries

---

## üìß Contact

**Shujaa** - [@imshujaa](https://github.com/imshujaa)

**Project Link**: [https://github.com/imshujaa/ML_Sensors_2026](https://github.com/imshujaa/ML_Sensors_2026)

---

## üìñ Citation

If you use this project in your research, please cite:

```bibtex
@software{ml_sensor_person_detection_2026,
  author = {Shujaa},
  title = {ML Sensor Person Detection: Production-Ready Edge AI},
  year = {2026},
  url = {https://github.com/imshujaa/ML_Sensors_2026}
}
```

---

<p align="center">
  <strong>Built with ‚ù§Ô∏è for Edge AI and Privacy-Preserving ML</strong>
</p>
